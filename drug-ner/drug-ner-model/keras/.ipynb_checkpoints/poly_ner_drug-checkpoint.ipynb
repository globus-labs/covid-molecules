{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drug NER with LSTM and CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, Bidirectional\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed, Input\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import gensim.models.word2vec as w2v\n",
    "import h5py\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from keras.utils import np_utils\n",
    "from keras_contrib.layers import CRF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import gensim\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __read_csv(file: str) -> tuple:\n",
    "    csv.field_size_limit(sys.maxsize)\n",
    "    sentences, labels = list(), list()\n",
    "    with open(file) as f:\n",
    "        reader = csv.reader(f, delimiter='|', quotechar='\"')\n",
    "        for item in reader:\n",
    "            sentences.append(eval(item[0]))\n",
    "            labels.append(eval(item[1]))\n",
    "    return (sentences, labels, len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input_files(train_file: str ='./train_val_seg.csv',\n",
    "                     test_file: str ='./test_seg.csv') -> tuple:\n",
    "    sentences, labels, train_num = __read_csv(train_file)\n",
    "    s, l, test_num  = __read_csv(test_file)\n",
    "    sentences.extend(s)\n",
    "    labels.extend(l)\n",
    "    return (sentences, labels, train_num, test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 120\n",
    "\n",
    "def fasttext_emb(sentences, modelpath, force=False):\n",
    "    if force == False and os.path.exists(modelpath) == True:\n",
    "        print (\"Word embedding model exists, skipping model %s.\" % modelpath)\n",
    "        return gensim.models.Word2Vec.load(modelpath)\n",
    "    \n",
    "    print (\"Generating word embedding model %s\" % modelpath)\n",
    "    model = gensim.models.FastText(sentences, size=EMBEDDING_DIM, window=5, min_count=1,\n",
    "                                   workers=4,sg=1, word_ngrams=5, iter=10, seed=23)\n",
    "    model.save(modelpath)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels, train_num, test_num = read_input_files()\n",
    "model = fasttext_emb(sentences, './emb/emb.bin')\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index=range(len(sentences)))\n",
    "df['sentences'] = sentences\n",
    "df['labels'] = labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finding proper maxlen:\")\n",
    "print(\"max:\", max(df['sentences'].apply(len)))\n",
    "print(\"99-percentile:\", df['sentences'].apply(len).quantile(0.99, interpolation='lower'))\n",
    "print(\"95-percentile:\", df['sentences'].apply(len).quantile(0.95, interpolation='lower'))\n",
    "print(\"90-percentile:\", df['sentences'].apply(len).quantile(0.90, interpolation='lower'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = df['sentences'].apply(len).quantile(0.99, interpolation='lower')\n",
    "\n",
    "def truncate_to_maxlen(l: list) -> list:\n",
    "    return l[:maxlen]\n",
    "\n",
    "df['sentences'] = df['sentences'].apply(truncate_to_maxlen)\n",
    "df['labels'] = df['labels'].apply(truncate_to_maxlen)\n",
    "assert max(df['sentences'].apply(len)) == max(df['labels'].apply(len)) == maxlen, \\\n",
    "        \"Error: Length mismatch after truncating!\"\n",
    "print(\"After truncating, new maxlen is:\", max(df['sentences'].apply(len)))\n",
    "df.index = range(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign an index to each unique word in the corpus\n",
    "if not os.path.exists('sci_ner_drug_model.h5'):\n",
    "    words_index = []\n",
    "    for i in sentences:\n",
    "        words_index.extend(i)\n",
    "    # value_counts() returns a Pandas.Series of unique words and their frequencies\n",
    "    words_index = pd.Series(words_index).value_counts()\n",
    "    # change the frequencies to range(), so that each word corresponds\n",
    "    # to an index number\n",
    "    words_index[:] = range(1, len(words_index) + 1)\n",
    "    # index 0 is reserved\n",
    "    words_index[''] = 0\n",
    "    pickle.dump(words_index, open(\"./emb/words_index.pkl\", \"wb\" ))\n",
    "else:\n",
    "    words_index = pickle.load(open(\"./emb/words_index.pkl\", \"rb\"))\n",
    "\n",
    "embedding_matrix = np.zeros((len(words_index)+1, EMBEDDING_DIM))\n",
    "for word, i in words_index.items():\n",
    "    try:\n",
    "        if word in word_vectors:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            # This shouldn't happen because min_count is set to 1 during WE training\n",
    "            embedding_vector = word_vectors[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass\n",
    "        # print(word)\n",
    "\n",
    "tag = pd.Series({'B':0, 'I':1, 'O':2, 'X':3})\n",
    "df['x'] = df['sentences'].apply(lambda s: np.array(list(words_index[s]) + [0] * (maxlen - len(s))))\n",
    "\n",
    "idx = list(range(len(df)))\n",
    "np.random.shuffle(idx)\n",
    "df = df.loc[idx]\n",
    "\n",
    "def trans_one(labels):\n",
    "    tmpLabels = map(lambda s: np_utils.to_categorical(s, 4), tag[labels].values.reshape((-1, 1)))\n",
    "    tmpLabels = list(tmpLabels)\n",
    "    tmpLabels.extend([np.array([[0, 0, 0, 1]])] * (maxlen - len(labels)))\n",
    "    return np.array(tmpLabels)\n",
    "df['y'] = df['labels'].apply(trans_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df['sentences']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM model with word-level info and CRF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#BiLSTM + CRF\n",
    "sequence = Input(shape=(maxlen,),dtype='int32')\n",
    "embedded = Embedding(len(words_index)+1, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                     input_length=maxlen, trainable=False, mask_zero=True)(sequence)\n",
    "blstm = Bidirectional(LSTM(32, return_sequences=True))(embedded)\n",
    "dropout = Dropout(0.1)(blstm)\n",
    "dense = TimeDistributed(Dense(32, activation='relu'))(dropout)\n",
    "\n",
    "crf = CRF(4)\n",
    "crf_output = crf(dense)\n",
    "\n",
    "model = Model(inputs=sequence, outputs=crf_output)\n",
    "\n",
    "model.compile(loss=crf.loss_function,\n",
    "              optimizer='adam',\n",
    "              metrics=[crf.accuracy])\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "#generate input data\n",
    "x = np.array(list(df['x']))\n",
    "y = np.array(list(df['y']))\n",
    "#adjust the shape of labels\n",
    "y = y.reshape((-1, maxlen, 4))\n",
    "\n",
    "x_train, y_train = x[:train_num], y[:train_num]\n",
    "x_test, y_test = x[train_num:], y[train_num:]\n",
    "\n",
    "if os.path.exists('sci_ner_drug_model.h5'):\n",
    "    print(\"Pretrained weights found, skipping training.\")\n",
    "    print(\"To retrain the model, delete the 'sci_ner_drug_model.h5' file in the current directory.\")\n",
    "    model.load_weights('sci_ner_drug_model.h5')\n",
    "else:\n",
    "    filepath=\"sci_ner_drug_model_best.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=64, callbacks=callbacks_list,\n",
    "                        # validation_data=(x_validation, y_validation)\n",
    "                        validation_split=0.4)\n",
    "    model.save('sci_ner_drug_model.h5')\n",
    "print(\"Evaluation on the withheld test set:\")\n",
    "acc = model.evaluate(x_test, y_test, batch_size = batch_size)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['crf_viterbi_accuracy'])\n",
    "plt.plot(history.history['val_crf_viterbi_accuracy'])\n",
    "plt.title('Model CRF accuracy')\n",
    "plt.ylabel('CRF Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['crf_viterbi_accuracy'])\n",
    "plt.plot(history.history['val_crf_viterbi_accuracy'])\n",
    "plt.title('Model CRF accuracy')\n",
    "plt.ylabel('CRF Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_keras]",
   "language": "python",
   "name": "conda-env-tf_keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
